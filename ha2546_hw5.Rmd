---
title: "Support Vector Machines"
author: "Hana Akbarnejad"
date: "5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(ggplot2)
library(readr)
library(patchwork)

library(caret)
library(e1071)
library(ISLR) # for data

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(2020)
```


```{r data, include=FALSE}

data(OJ)
oj_data = OJ
oj_data = oj_data %>% 
  janitor::clean_names() %>% 
  mutate(
    purchase = as.factor(purchase)
  )

set.seed(2020)
train_rows = createDataPartition(y = oj_data$purchase,
                                 p = 0.746729,
                                 list = FALSE)
train_data = oj_data[train_rows,]
test_data = oj_data[-train_rows,]

ctrl = trainControl(method = "cv")
```

### First I fit a support vector classifier (linear kernel) to the training data.
```{r linear_kernel}

set.seed(2020)
# SWM with linear kernel, caret
svml_fit = train(purchase~.,
                  data = train_data,
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-6,4,len=60))),
                  trControl = ctrl)

ggplot(svml_fit, highlight = TRUE)

svml_fit$bestTune

summary(svml_fit)
```

### Then, I fit a support vector machine with a radial kernel to the training data.
```{r radial_kernel}

set.seed(2020)
svmr_grid = expand.grid(C = exp(seq(-4,5,len=10)),
                        sigma = exp(seq(-8,-3,len=5)))

svmr_fit = train(purchase~., train_data,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr_grid,
                  trControl = ctrl)

ggplot(svmr_fit, highlight = TRUE)

svmr_fit$bestTune

summary(svmr_fit)
```

### Then, I look at training error rates of both SVM kernels
```{r train_error}

set.seed(2020)
resamp = resamples(list(svm_radial = svmr_fit, svm_linear = svml_fit))
summary(resamp)
bwplot(resamp)
```

The summary of *resamples()* function show that SVM with linear kernel has training error rate of `r round((1-0.8249269)*100, 2)`% and SVM with radial kernel has training error rate of `r round((1-0.85)*100, 2)`%. So, if we want to select one of these models, we would prefer radial kernel with lower mean error. However, we can observe that the values are pretty close.


### Finally, I look at test performance of both SVM kernels
```{r test_error}

#looking at test performance of linear kernel
svml_pred = predict(svml_fit, newdata = test_data)

confusionMatrix(data = svml_pred,
                reference = oj_data$purchase[-train_rows])

#looking at test performance of radial kernel
svmr_pred = predict(svmr_fit, newdata = test_data)

confusionMatrix(data = svmr_pred,
                reference = oj_data$purchase[-train_rows])
```

Looking at confusion matrix of two SVM models, we can see that test error rate of the one with linear kernel is `r round((1-0.863)*100, 2)`% and the test error rate of the one with radial kernel is `r round((1-0.8481)*100, 2)`%. So, it seems that linear kernel has a better model performance on test dataset.
